"""
Privacy attack simulation module.
Implements membership inference and prompt extraction attacks to detect data leakage.
"""

import torch
import json
import random
from pathlib import Path
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from tqdm import tqdm
from typing import List, Dict, Tuple

import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import (
    DATA_DIR, MODELS_DIR, ATTACK_PROMPTS, 
    NUM_ATTACK_SAMPLES, MAX_LENGTH, RANDOM_SEED
)


class PrivacyAttacker:
    """Simulate privacy attacks on trained models"""
    
    def __init__(self, model_path, device=None):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Load model and tokenizer
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)
        self.model = GPT2LMHeadModel.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"Loaded model from {model_path}")
    
    def load_private_records(self, filename='train_private_records.json'):
        """Load tracked private records"""
        records_file = DATA_DIR / filename
        with open(records_file, 'r') as f:
            records = json.load(f)
        return records
    
    def generate_text(self, prompt: str, max_length: int = 50, 
                     num_return_sequences: int = 1) -> List[str]:
        """Generate text from prompt"""
        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                min_length=max(10, len(inputs['input_ids'][0]) + 5),  # Force minimum generation
                num_return_sequences=num_return_sequences,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=0.7,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=None  # Disable early stopping at eos
            )
        
        generated_texts = [
            self.tokenizer.decode(output, skip_special_tokens=True) 
            for output in outputs
        ]
        
        return generated_texts
    
    def check_pii_leakage(self, generated_text: str, private_record: Dict) -> Dict[str, bool]:
        """Check if generated text contains PII from a private record"""
        leakage = {
            'name': private_record['name'].lower() in generated_text.lower(),
            'email': private_record['email'].lower() in generated_text.lower(),
            'ssn': private_record['ssn'] in generated_text,
            'phone': private_record['phone'] in generated_text,
        }
        
        return leakage
    
    def prompt_extraction_attack(self, num_samples: int = NUM_ATTACK_SAMPLES, 
                                records_filename: str = 'train_private_records.json') -> Dict:
        """
        Attempt to extract private information using targeted prompts
        """
        print("\nüîç Running prompt extraction attack...")
        
        private_records = self.load_private_records(filename=records_filename)
        random.seed(RANDOM_SEED)
        sampled_records = random.sample(private_records, min(num_samples, len(private_records)))
        
        leakage_stats = {
            'name': 0,
            'email': 0,
            'ssn': 0,
            'phone': 0,
            'any_leakage': 0,
            'total_attempts': 0
        }
        
        leaked_examples = []
        
        for record in tqdm(sampled_records, desc="Testing prompts"):
            for prompt in ATTACK_PROMPTS:
                generated_texts = self.generate_text(prompt, max_length=100, num_return_sequences=3)
                
                for gen_text in generated_texts:
                    leakage = self.check_pii_leakage(gen_text, record)
                    leakage_stats['total_attempts'] += 1
                    
                    if any(leakage.values()):
                        leakage_stats['any_leakage'] += 1
                        leaked_examples.append({
                            'prompt': prompt,
                            'generated': gen_text,
                            'leaked_fields': [k for k, v in leakage.items() if v]
                        })
                    
                    for key in ['name', 'email', 'ssn', 'phone']:
                        if leakage[key]:
                            leakage_stats[key] += 1
        
        # Calculate leakage rate
        leakage_rate = (leakage_stats['any_leakage'] / leakage_stats['total_attempts']) * 100
        
        results = {
            'attack_type': 'prompt_extraction',
            'leakage_rate': leakage_rate,
            'statistics': leakage_stats,
            'leaked_examples': leaked_examples[:5]  # Store first 5 examples
        }
        
        print(f"\nüìä Prompt Extraction Results:")
        print(f"   - Leakage Rate: {leakage_rate:.2f}%")
        print(f"   - Total Attempts: {leakage_stats['total_attempts']}")
        print(f"   - Leakages Detected: {leakage_stats['any_leakage']}")
        
        return results
    
    def membership_inference_attack(self, num_samples: int = NUM_ATTACK_SAMPLES,
                                   records_filename: str = 'train_private_records.json') -> Dict:
        """
        Test if model can reproduce training examples (membership inference)
        """
        print("\nüîç Running membership inference attack...")
        
        private_records = self.load_private_records(filename=records_filename)
        random.seed(RANDOM_SEED)
        sampled_records = random.sample(private_records, min(num_samples, len(private_records)))
        
        correct_inferences = 0
        inference_scores = []
        
        for record in tqdm(sampled_records, desc="Testing membership"):
            # Use part of the original text as prompt
            original_text = record['text']
            words = original_text.split()
            
            if len(words) < 5:
                continue
            
            # Use first few words as prompt
            prompt = ' '.join(words[:3])
            target = ' '.join(words[3:])
            
            # Generate continuation
            generated = self.generate_text(prompt, max_length=len(original_text.split()) + 10)
            
            # Check if generation contains the target continuation
            for gen_text in generated:
                # Remove prompt from generated text
                gen_continuation = gen_text[len(prompt):].strip()
                
                # Calculate overlap
                overlap = sum(1 for word in target.split() if word.lower() in gen_continuation.lower())
                score = overlap / len(target.split()) if target.split() else 0
                inference_scores.append(score)
                
                if score > 0.5:  # Threshold for "memorization"
                    correct_inferences += 1
                    break
        
        inference_rate = (correct_inferences / len(sampled_records)) * 100
        avg_score = sum(inference_scores) / len(inference_scores) if inference_scores else 0
        
        results = {
            'attack_type': 'membership_inference',
            'inference_rate': inference_rate,
            'average_score': avg_score,
            'total_samples': len(sampled_records),
            'memorized_samples': correct_inferences
        }
        
        print(f"\nüìä Membership Inference Results:")
        print(f"   - Inference Rate: {inference_rate:.2f}%")
        print(f"   - Average Overlap Score: {avg_score:.3f}")
        print(f"   - Memorized Samples: {correct_inferences}/{len(sampled_records)}")
        
        return results
    
    def run_all_attacks(self) -> Dict:
        """Run all privacy attacks and aggregate results"""
        print("\n" + "="*60)
        print("üéØ PRIVACY ATTACK SIMULATION")
        print("="*60)
        
        prompt_results = self.prompt_extraction_attack()
        membership_results = self.membership_inference_attack()
        
        combined_results = {
            'prompt_extraction': prompt_results,
            'membership_inference': membership_results,
            'overall_privacy_risk': (
                prompt_results['leakage_rate'] + membership_results['inference_rate']
            ) / 2
        }
        
        print("\n" + "="*60)
        print(f"üö® Overall Privacy Risk Score: {combined_results['overall_privacy_risk']:.2f}%")
        print("="*60)
        
        return combined_results


def main():
    """Run privacy attacks on baseline model"""
    model_path = MODELS_DIR / "baseline_model"
    
    if not model_path.exists():
        print("‚ùå Baseline model not found. Please train the baseline model first.")
        return
    
    attacker = PrivacyAttacker(model_path)
    results = attacker.run_all_attacks()
    
    # Save results
    output_file = MODELS_DIR / "baseline_attack_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ Results saved to {output_file}")


if __name__ == "__main__":
    main()
